<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">0</article-id>
<article-id pub-id-type="doi">N/A</article-id>
<title-group>
<article-title>cuallee: A python package for data quality checks across
multiple DataFrame APIs</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1937-8006</contrib-id>
<name>
<surname>Vazquez</surname>
<given-names>Herminio</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8249-7182</contrib-id>
<name>
<surname>Grosboillot</surname>
<given-names>Virginie</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Independent Researcher, Mexico</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Swiss Federal Institute of Technology (ETH)</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-12-11">
<day>11</day>
<month>12</month>
<year>2022</year>
</pub-date>
<volume>¿VOL?</volume>
<issue>¿ISSUE?</issue>
<fpage>¿PAGE?</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>python</kwd>
<kwd>data quality</kwd>
<kwd>data checks</kwd>
<kwd>data unit tests</kwd>
<kwd>data pipelines</kwd>
<kwd>data validation</kwd>
<kwd>data observability</kwd>
<kwd>data lake</kwd>
<kwd>pyspark</kwd>
<kwd>duckdb</kwd>
<kwd>pandas</kwd>
<kwd>snowpark</kwd>
<kwd>polars</kwd>
<kwd>big data</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>In today’s world, where vast amounts of data are generated and
  collected daily, and where data heavily influence business, political,
  and societal decisions, it is crucial to evaluate the quality of the
  data used for analysis, decision-making, and reporting. This involves
  understanding how reliable and trustworthy the data are. To address
  this need, we have created <monospace>cuallee</monospace>, a Python
  package for assessing data quality. <monospace>cuallee</monospace> is
  designed to be dataframe-agnostic, offering an intuitive and
  user-friendly API for describing checks across the most popular
  dataframe implementations such as PySpark, Pandas, Snowpark, Polars,
  DuckDB and BigQuery. Currently, <monospace>cuallee</monospace> offers
  over 50 checks to help users evaluate the quality of their data.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>For data engineers and data scientists, maintaining a consistent
  workflow involves operating in hybrid environments, where they develop
  locally before transitioning data pipelines and analyses to
  cloud-based environments. Whilst working in local environments
  typically allows them to fit data sets in memory, moving workloads to
  cloud environments involve operating with full scale data that
  requires a different computing framework
  (<xref alt="Schelter et al., 2018" rid="ref-10.14778U002F3229863.3229867" ref-type="bibr">Schelter
  et al., 2018</xref>), i.e. distributed computing, parallelization, and
  horizontal scaling. <monospace>cuallee</monospace> accomodates the
  testing activities required by this shift in computing frameworks, in
  both local and remote environments, without the need to rewrite test
  scenarios or employ different testing approaches for assessing various
  quality dimensions of the data
  (<xref alt="Fadlallah et al., 2023b" rid="ref-10.1145U002F3603707" ref-type="bibr">Fadlallah
  et al., 2023b</xref>).</p>
  <p>An additional argument is related to the rapid evolution of the
  data ecosystem
  (<xref alt="Fadlallah et al., 2023a" rid="ref-10.1145U002F3603706" ref-type="bibr">Fadlallah
  et al., 2023a</xref>). Organizations and data teams are constantly
  seeking ways to improve, whether through cost-effective solutions or
  by integrating new capabilities into their data operations. However,
  this pursuit presents new challenges when migrating workloads from one
  technology to another. As information technology and data strategies
  become more resilient against vendor lock-ins, they turn to
  technologies that enable seamless operation across platforms, avoiding
  the chaos of fully re-implementing data products. In essence, with
  <monospace>cuallee</monospace> no data testing strategy needs to be
  rewritten or reformulated due to platform changes.</p>
  <p>One last argument in favor of using a quality tool such as
  <monospace>cuallee</monospace> is the need to integrate quality
  procedures into the early stages of data product development. Whether
  in industry or academia, there’s often a tendency to prioritize
  functional aspects over quality, leading to less time being dedicated
  to quality activities. By providing a clear, easy-to-use, and
  adaptable programming interface for data quality, teams can
  incorporate quality into their development process, promoting a
  proactive approach of building quality in rather than relying solely
  on testing to ensure quality.</p>
</sec>
<sec id="methods">
  <title>Methods</title>
  <p><monospace>cuallee</monospace> employs a heuristic-based approach
  to define quality rules for each dataset. This prevents the
  inadvertent duplication of quality predicates, thus reducing the
  likelihood of human error in defining rules with identical predicates.
  Several studies have been conducted on the efficiency of these rules,
  including auto-validation and auto-definition using profilers
  (<xref alt="Tu et al., 2023" rid="ref-10.1145U002F3580305.3599776" ref-type="bibr">Tu
  et al., 2023</xref>).</p>
</sec>
<sec id="checks">
  <title>Checks</title>
  <p>In <monospace>cuallee</monospace>, checks serve as the fundamental
  concept. These checks
  (<xref alt="Table 1" rid="tabU003Achecks">Table 1</xref>) are
  implemented by <bold>rules</bold>, which specify <italic>quality
  predicates</italic>. These predicates, when aggregated, form the
  criteria used to evaluate the quality of a dataset. Efforts to
  establish a universal quality metric
  (<xref alt="Pleimling et al., 2022" rid="ref-10.1145U002F3529190.3529222" ref-type="bibr">Pleimling
  et al., 2022</xref>) typically involve using statistics and combining
  dimensions to derive a single reference value that encapsulates
  overall quality attributes.</p>
  <table-wrap>
    <caption>
      <p>List and description of the currently available
      <styled-content id="tabU003Achecks"></styled-content></p>
    </caption>
    <table>
      <colgroup>
        <col width="32%" />
        <col width="50%" />
        <col width="18%" />
      </colgroup>
      <thead>
        <tr>
          <th>Check</th>
          <th>Description</th>
          <th>DataType</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><monospace>is_complete</monospace></td>
          <td>Zero <monospace>nulls</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>is_unique</monospace></td>
          <td>Zero <monospace>duplicates</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>is_primary_key</monospace></td>
          <td>Zero <monospace>duplicates</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>are_complete</monospace></td>
          <td>Zero <monospace>nulls</monospace> on group of columns</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>are_unique</monospace></td>
          <td>Composite primary key check</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>is_composite_key</monospace></td>
          <td>Zero duplicates on multiple columns</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>is_greater_than</monospace></td>
          <td><monospace>col &gt; x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_positive</monospace></td>
          <td><monospace>col &gt; 0</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_negative</monospace></td>
          <td><monospace>col &lt; 0</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_greater_or_equal_than</monospace></td>
          <td><monospace>col &gt;= x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_less_than</monospace></td>
          <td><monospace>col &lt; x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_less_or_equal_than</monospace></td>
          <td><monospace>col &lt;= x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_equal_than</monospace></td>
          <td><monospace>col == x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_contained_in</monospace></td>
          <td><monospace>col in [a, b, c, ...]</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>is_in</monospace></td>
          <td>Alias of <monospace>is_contained_in</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>not_contained_in</monospace></td>
          <td><monospace>col not in [a, b, c, ...]</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>not_in</monospace></td>
          <td>Alias of <monospace>not_contained_in</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>is_between</monospace></td>
          <td><monospace>a &lt;= col &lt;= b</monospace></td>
          <td><italic>numeric, date</italic></td>
        </tr>
        <tr>
          <td><monospace>has_pattern</monospace></td>
          <td>Matching a pattern defined as a
          <monospace>regex</monospace></td>
          <td><italic>string</italic></td>
        </tr>
        <tr>
          <td><monospace>is_legit</monospace></td>
          <td>String not null &amp; not empty
          <monospace>^\S$</monospace></td>
          <td><italic>string</italic></td>
        </tr>
        <tr>
          <td><monospace>has_min</monospace></td>
          <td><monospace>min(col) == x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>has_max</monospace></td>
          <td><monospace>max(col) == x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>has_std</monospace></td>
          <td><monospace>σ(col) == x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>has_mean</monospace></td>
          <td><monospace>μ(col) == x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>has_sum</monospace></td>
          <td><monospace>Σ(col) == x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>has_percentile</monospace></td>
          <td><monospace>%(col) == x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>has_cardinality</monospace></td>
          <td><monospace>count(distinct(col)) == x</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>has_max_by</monospace></td>
          <td>A utilitary predicate for
          <monospace>max(col_a) == x for max(col_b)</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>has_min_by</monospace></td>
          <td>A utilitary predicate for
          <monospace>min(col_a) == x for min(col_b)</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>has_correlation</monospace></td>
          <td>Finds correlation between <monospace>0..1</monospace> on
          <monospace>corr(col_a, col_b)</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>has_entropy</monospace></td>
          <td>Calculates the entropy of a column
          <monospace>entropy(col) == x</monospace> for classification
          problems</td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_inside_iqr</monospace></td>
          <td>Verifies column values reside inside limits of
          interquartile range
          <monospace>Q1 &lt;= col &lt;= Q3</monospace> used on
          anomalies.</td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_in_millions</monospace></td>
          <td><monospace>col &gt;= 1e6</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_in_billions</monospace></td>
          <td><monospace>col &gt;= 1e9</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_t_minus_1</monospace></td>
          <td>For date fields confirms 1 day ago
          <monospace>t-1</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_t_minus_2</monospace></td>
          <td>For date fields confirms 2 days ago
          <monospace>t-2</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_t_minus_3</monospace></td>
          <td>For date fields confirms 3 days ago
          <monospace>t-3</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_t_minus_n</monospace></td>
          <td>For date fields confirms n days ago
          <monospace>t-n</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_today</monospace></td>
          <td>For date fields confirms day is current date
          <monospace>t-0</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_yesterday</monospace></td>
          <td>For date fields confirms 1 day ago
          <monospace>t-1</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_weekday</monospace></td>
          <td>For date fields confirms day is between
          <monospace>Mon-Fri</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_weekend</monospace></td>
          <td>For date fields confirms day is between
          <monospace>Sat-Sun</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_monday</monospace></td>
          <td>For date fields confirms day is
          <monospace>Mon</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_tuesday</monospace></td>
          <td>For date fields confirms day is
          <monospace>Tue</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_wednesday</monospace></td>
          <td>For date fields confirms day is
          <monospace>Wed</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_thursday</monospace></td>
          <td>For date fields confirms day is
          <monospace>Thu</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_friday</monospace></td>
          <td>For date fields confirms day is
          <monospace>Fri</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_saturday</monospace></td>
          <td>For date fields confirms day is
          <monospace>Sat</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_sunday</monospace></td>
          <td>For date fields confirms day is
          <monospace>Sun</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_schedule</monospace></td>
          <td>For date fields confirms time windows
          i.e. <monospace>9:00 - 17:00</monospace></td>
          <td><italic>timestamp</italic></td>
        </tr>
        <tr>
          <td><monospace>is_daily</monospace></td>
          <td>Can verify daily continuity on date fields by default.
          <monospace>[2,3,4,5,6]</monospace> which represents
          <monospace>Mon-Fri</monospace> in PySpark. However new
          schedules can be used for custom date continuity</td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>has_workflow</monospace></td>
          <td>Adjacency matrix validation on
          <monospace>3-column</monospace> graph, based on
          <monospace>group</monospace>, <monospace>event</monospace>,
          <monospace>order</monospace> columns.</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>satisfies</monospace></td>
          <td>An open <monospace>SQL expression</monospace> builder to
          construct custom checks</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>validate</monospace></td>
          <td>The ultimate transformation of a check with a
          <monospace>dataframe</monospace> input for validation</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>iso.iso_4217</monospace></td>
          <td>currency compliant <monospace>ccy</monospace></td>
          <td><italic>string</italic></td>
        </tr>
        <tr>
          <td><monospace>iso.iso_3166</monospace></td>
          <td>country compliant <monospace>country</monospace></td>
          <td><italic>string</italic></td>
        </tr>
        <tr>
          <td><monospace>Control.completeness</monospace></td>
          <td>Zero <monospace>nulls</monospace> all columns</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>Control.percentage_fill</monospace></td>
          <td><monospace>% rows</monospace> not empty</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>Control.percentage_empty</monospace></td>
          <td><monospace>% rows</monospace> empty</td>
          <td><italic>agnostic</italic></td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-10.1145U002F3603707">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fadlallah</surname><given-names>Hadi</given-names></name>
        <name><surname>Kilany</surname><given-names>Rima</given-names></name>
        <name><surname>Dhayne</surname><given-names>Houssein</given-names></name>
        <name><surname>El Haddad</surname><given-names>Rami</given-names></name>
        <name><surname>Haque</surname><given-names>Rafiqul</given-names></name>
        <name><surname>Taher</surname><given-names>Yehia</given-names></name>
        <name><surname>Jaber</surname><given-names>Ali</given-names></name>
      </person-group>
      <article-title>Context-aware big data quality assessment: A scoping review</article-title>
      <source>Journal of Data and Information Quality</source>
      <year iso-8601-date="2023-06">2023</year><month>06</month>
      <volume>15</volume>
      <pub-id pub-id-type="doi">10.1145/3603707</pub-id>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.1145U002F3603706">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fadlallah</surname><given-names>Hadi</given-names></name>
        <name><surname>Kilany</surname><given-names>Rima</given-names></name>
        <name><surname>Dhayne</surname><given-names>Houssein</given-names></name>
        <name><surname>El Haddad</surname><given-names>Rami</given-names></name>
        <name><surname>Haque</surname><given-names>Rafiqul</given-names></name>
        <name><surname>Taher</surname><given-names>Yehia</given-names></name>
        <name><surname>Jaber</surname><given-names>Ali</given-names></name>
      </person-group>
      <article-title>BIGQA: Declarative big data quality assessment</article-title>
      <source>Journal of Data and Information Quality</source>
      <year iso-8601-date="2023-06">2023</year><month>06</month>
      <volume>15</volume>
      <pub-id pub-id-type="doi">10.1145/3603706</pub-id>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.1145U002F3580305.3599776">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Tu</surname><given-names>Dezhan</given-names></name>
        <name><surname>He</surname><given-names>Yeye</given-names></name>
        <name><surname>Cui</surname><given-names>Weiwei</given-names></name>
        <name><surname>Ge</surname><given-names>Song</given-names></name>
        <name><surname>Zhang</surname><given-names>Haidong</given-names></name>
        <name><surname>Han</surname><given-names>Shi</given-names></name>
        <name><surname>Zhang</surname><given-names>Dongmei</given-names></name>
        <name><surname>Chaudhuri</surname><given-names>Surajit</given-names></name>
      </person-group>
      <article-title>Auto-validate by-history: Auto-program data quality constraints to validate recurring data pipelines</article-title>
      <source>Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2023">2023</year>
      <isbn>9798400701030</isbn>
      <uri>https://doi.org/10.1145/3580305.3599776</uri>
      <pub-id pub-id-type="doi">10.1145/3580305.3599776</pub-id>
      <fpage>4991</fpage>
      <lpage>5003</lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.14778U002F3229863.3229867">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Schelter</surname><given-names>Sebastian</given-names></name>
        <name><surname>Lange</surname><given-names>Dustin</given-names></name>
        <name><surname>Schmidt</surname><given-names>Philipp</given-names></name>
        <name><surname>Celikel</surname><given-names>Meltem</given-names></name>
        <name><surname>Biessmann</surname><given-names>Felix</given-names></name>
        <name><surname>Grafberger</surname><given-names>Andreas</given-names></name>
      </person-group>
      <article-title>Automating large-scale data quality verification</article-title>
      <source>Proc. VLDB Endow.</source>
      <publisher-name>VLDB Endowment</publisher-name>
      <year iso-8601-date="2018-08">2018</year><month>08</month>
      <volume>11</volume>
      <issue>12</issue>
      <issn>2150-8097</issn>
      <uri>https://doi.org/10.14778/3229863.3229867</uri>
      <pub-id pub-id-type="doi">10.14778/3229863.3229867</pub-id>
      <fpage>1781</fpage>
      <lpage>1794</lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.1145U002F3529190.3529222">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Pleimling</surname><given-names>Xavier</given-names></name>
        <name><surname>Shah</surname><given-names>Vedant</given-names></name>
        <name><surname>Lourentzou</surname><given-names>Ismini</given-names></name>
      </person-group>
      <article-title>[Data] quality lies in the eyes of the beholder</article-title>
      <source>Proceedings of the 15th international conference on PErvasive technologies related to assistive environments</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2022">2022</year>
      <isbn>9781450396318</isbn>
      <uri>https://doi.org/10.1145/3529190.3529222</uri>
      <pub-id pub-id-type="doi">10.1145/3529190.3529222</pub-id>
      <fpage>118</fpage>
      <lpage>124</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
